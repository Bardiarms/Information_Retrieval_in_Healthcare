# Medical Question Summarization using Paraphrased Data

## Introduction
The goal in this project is to develop an effective approach for summarizing consumer health questions (CHQ). CHQs are often lengthy and complex, containing multiple layers of information, which makes automatic summarization critical for enhancing information retrieval in healthcare systems.

This project uses advanced machine translation and NLP techniques to generate diverse paraphrases of the original questions and uses these paraphrases to improve question summarization. Additionally, evaluation metrics such as ROUGE and BLEU are applied to assess the quality of the generated summaries.

---

## Key Steps and Implementation

### 1. Dataset Preparation
The MeQSum dataset which consists health-related questions and summaries, was used as the base data for this project.

#### Key Preprocessing Steps:
- Removed special characters, numbers, and converted text to lowercase.
- Filtered extremely short and long questions.
- Removed common English stopwords.

### 2. Round-Trip Translation (RTT)
We used both pre-trained MarianMT models and the Google Translate API to generate paraphrased CHQs.

#### Translation Process:
- Forward translation of questions to five pivot languages: Spanish (es), German (de), Italian (it), Chinese (zh), and French (fr).
- Backward translation to English.
- This process generated multiple paraphrased versions of each CHQ.

### 3. Question Selection
We applied quality measures to select subsets of paraphrased questions.

#### FrechÂ´et Question Distance (FQD)
Calculated using cosine similarity between the original and paraphrased question embeddings generated by BERT.

#### Precision Recall Question Distance (PRQD)
Calculated precision and recall by comparing the overlap of two probability distributions on an element-by-element basis. This approach captures how much of one distribution is "covered" by the other.

---

### 4. Summarization
We employed the T5 transformer model to generate question summaries.

#### Summarization Details:
- Raw CHQs were summarized.
- Paraphrased CHQs generated by MarianMT and Google Translate were also summarized.

---

### 5. Evaluation
To evaluate the quality of the generated summaries, we applied the following metrics:

- **ROUGE:** Measures overlap between generated and reference summaries.
- **BLEU:** Evaluates the n-gram overlap between generated summaries and reference text.


## Evaluation Results

**Results for paraphrases generated by MarianMT model and selected by FQD measure:**

- **ROUGE Scores:**  
  `{'rouge1': np.float64(0.15776680678908958), 'rouge2': np.float64(0.04515783185029319), 'rougeL': np.float64(0.1491152658459397), 'rougeLsum': np.float64(0.14908327632133256)}`

- **BLEU Score:**  
  `{'bleu': 0.009286322633011625, 'precisions': [0.09871328946076025, 0.02784542085759661, 0.00539609644087256, 0.000501378791677112], 'brevity_penalty': 1.0, 'length_ratio': 2.771848625102096, 'translation_length': 10181, 'reference_length': 3673}`

**Results for paraphrases generated by MarianMT model and selected by PRQD measure:**

- **ROUGE Scores:**  
  `{'rouge1': np.float64(0.10467084748065916), 'rouge2': np.float64(0.03254229693640968), 'rougeL': np.float64(0.09860225871213371), 'rougeLsum': np.float64(0.0986255581711745)}`

- **BLEU Score:**  
  `{'bleu': 0.006578449912156576, 'precisions': [0.05722143364088006, 0.017103815113719736, 0.003891419893697798, 0.0004917387883556255], 'brevity_penalty': 1.0, 'length_ratio': 6.1377620473727195, 'translation_length': 22544, 'reference_length': 3673}`

**Results for paraphrases generated by Google Translate and selected by FQD measure:**

- **ROUGE Scores:**  
  `{'rouge1': np.float64(0.16934945447715366), 'rouge2': np.float64(0.056249468423383964), 'rougeL': np.float64(0.16083201174958245), 'rougeLsum': np.float64(0.16085861334727197)}`

- **BLEU Score:**  
  `{'bleu': 0.010993821836385236, 'precisions': [0.09743395511788656, 0.030127226463104326, 0.005941247662009022, 0.0008376211559171952], 'brevity_penalty': 1.0, 'length_ratio': 2.875306289136945, 'translation_length': 10561, 'reference_length': 3673}`

**Results for paraphrases generated by Google Translate and selected by PRQD measure:**

- **ROUGE Scores:**  
  `'rouge1': np.float64(0.10821114464690451), 'rouge2': np.float64(0.037774293049894145), 'rougeL': np.float64(0.10431176762453298), 'rougeLsum': np.float64(0.10419236865924247)}`

- **BLEU Score:**  
  `{'bleu': 0.006705637803310368, 'precisions': [0.05687590711175617, 0.01853415915915916, 0.004228227060653188, 0.00045362903225806454], 'brevity_penalty': 1.0, 'length_ratio': 6.00272257010618, 'translation_length': 22048, 'reference_length': 3673}`

**Result for summaries generated from raw questions:**

- **ROUGE Scores:**  
  `rouge1': np.float64(0.23740517483724086), 'rouge2': np.float64(0.08975453339463886), 'rougeL': np.float64(0.22635419425117156), 'rougeLsum': np.float64(0.2267578244035588)}`

- **BLEU Score:**  
  `{'bleu': 0.04056550439204956, 'precisions': [0.2190063810391978, 0.08625410733844469, 0.02638793694311172, 0.005432322317790856], 'brevity_penalty': 1.0, 'length_ratio': 1.1946637625918868, 'translation_length': 4388, 'reference_length': 3673}`
---

## Results and Insights
- **Paraphrase Diversity:** RTT using multiple pivot languages successfully generated diverse paraphrases, enhancing the dataset's diversity.

- **Evaluation Scores:** Summarization based on Google Translate paraphrases yielded higher ROUGE scores, which shows better alignment with reference summaries. Raw summarization was by far better than summaries based on RTT generated paraphrases on both ROUGE and BLEU metrics.
Summaries with paraphrases selected by FQD metric also had better ROUGE and BLEU socres than those selected by PRQD metric.

---

## Key Challenges
1. Handling noisy or invalid paraphrases during RTT can be considered one of the challenges that should be dealt with. 
2. Ensuring meaningful summary content despite language translation artifacts is another one.
3. Using several libraries together and handling outputs to be compatible and usable in every step was also frustrating.
4. Translation process on dataset was time-consuming slow and needed several hours to be done, specially when using MarianMT model. 

---

## Conclusion
This project shows the importance of using paraphrased data for improving CHQ summarization. With the help of RTT and advanced evaluation metrics, we demonstrate a solid framework for handling complex health-related queries.

---

## How to Run the Project

### Prerequisites
- Python 3.x
- Required libraries: `transformers`, `torch`, `nltk`, `deep-translator`, `evaluate`, `sentence-transformers`, `numpy`

### Setup
1. Install the dependencies:
   ```bash
   pip install transformers torch nltk deep-translator evaluate sentence-transformers
   ```

2. Run the preprocessing and translation steps:
   ```python
   # Load and clean the dataset
   # Apply RTT using MarianMT and Google Translate
   ```

3. Generate summaries and compute evaluation scores:
   ```python
   # Summarize raw and paraphrased CHQs
   # Evaluate using ROUGE and BLEU metrics
   ```